{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c74e22",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1685956833801,
     "user": {
      "displayName": "Javier Quesada García",
      "userId": "08428804086464618311"
     },
     "user_tz": -120
    },
    "id": "25c74e22"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random \n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l0Z_xOP7gRD8",
   "metadata": {
    "id": "l0Z_xOP7gRD8"
   },
   "outputs": [],
   "source": [
    "\n",
    "def scrap_request(url, user_agents):\n",
    "    \"\"\"\n",
    "    Creates a requests.Session and then\n",
    "    makes a GET request using the provided 'url' and 'user_agents'\n",
    "    It will pick a random user_agent from the user_agents list provided\n",
    "    It has a built-in random delay to avoid bot detection when web scrapping\n",
    "    It returns the session.get request\n",
    "    Will return None if some exception arises\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        random_user_agent = random.choice(user_agents)\n",
    "\n",
    "        session = requests.Session()\n",
    "\n",
    "        response = session.get(url, headers = {\"User-Agent\":random_user_agent} )\n",
    "\n",
    "        delay = random.uniform(1,3)\n",
    "\n",
    "        time.sleep(delay)\n",
    "\n",
    "        return response\n",
    "    \n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e615c7c",
   "metadata": {
    "id": "1e615c7c"
   },
   "outputs": [],
   "source": [
    "def scrap_car_urls(url, user_agents):\n",
    "    \"\"\"\n",
    "    This functions scraps each car announcement URL from a autocasion.com page\n",
    "    It uses scrap_request function for the GET request\n",
    "    It will return a list with the complete link for each car announcement\n",
    "    Will return None if some exception arises\n",
    "    \"\"\"\n",
    "    try:\n",
    "        page_bs = BeautifulSoup(scrap_request(url, user_agents).text, \"lxml\")\n",
    "        \n",
    "        cars_list = page_bs.find_all(\"article\", {'class': re.compile(\"anuncio\")})\n",
    "        \n",
    "        starting_url = \"https://www.autocasion.com\"\n",
    "        \n",
    "        links_list = [starting_url + link.find(\"a\")[\"href\"]\n",
    "                  for link in cars_list]\n",
    "        \n",
    "        return links_list\n",
    "    \n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe81ddb",
   "metadata": {
    "id": "9fe81ddb",
    "outputId": "3b514d50-64d3-4636-f4b4-162e4c45beb2"
   },
   "outputs": [],
   "source": [
    "def scrap_car_details(url, user_agents):\n",
    "    \"\"\"\n",
    "    This function scraps a car announcement url from autocasion.com\n",
    "    It uses scrap_request function for the GET request\n",
    "    It returns a dictionary with its main characteristics (brand, model, transmission...)\n",
    "    Will return None if some exception arises\n",
    "    \"\"\"\n",
    "    try:\n",
    "        car_dict = {}\n",
    "        \n",
    "        car_info_bs = BeautifulSoup(scrap_request(url, user_agents).text, \"lxml\")\n",
    "        \n",
    "        #car announcement url\n",
    "        car_dict[\"link\"] = url\n",
    "        \n",
    "        #car brand\n",
    "        car_dict[\"marca\"] = car_info_bs.find(\"div\", class_=\"bloque paginacion-ficha\").find_all(itemprop=\"name\")[1].text\n",
    "        \n",
    "        #province where the car is being sold\n",
    "        car_dict[\"provincia\"] = car_info_bs.find(\"div\", class_=\"bloque paginacion-ficha\").find_all(itemprop=\"name\")[2].text\n",
    "        \n",
    "        #car model\n",
    "        car_dict[\"modelo\"] = car_info_bs.find(\"div\", class_=\"bloque paginacion-ficha\").find_all(itemprop=\"name\")[3].text\n",
    "        \n",
    "        #car price\n",
    "        car_dict[\"precio\"] = int(\"\".join(re.findall(\"\\d\",car_info_bs.find(\"div\", class_=\"precio\").find(\"span\").text.strip())))\n",
    "        \n",
    "        #year in which the car was registrated\n",
    "        car_dict[\"matriculacion\"] = car_info_bs.find(\n",
    "            \"ul\", class_=\"datos-basicos-ficha\").find(\n",
    "            text= re.compile(\"Fecha de matriculación\")\n",
    "        ).find_next().text[-4:]\n",
    "        \n",
    "        #fuel type\n",
    "        car_dict[\"combustible\"] =  car_info_bs.find(\n",
    "            \"ul\", class_=\"datos-basicos-ficha\").find(\n",
    "            text = re.compile(\"Combustible\")).find_next().text.strip()\n",
    "        \n",
    "        #car km\n",
    "        car_dict[\"kilometros\"] =  car_info_bs.find(\n",
    "            \"ul\", class_=\"datos-basicos-ficha\").find(\n",
    "            text = re.compile(\"Kilómetros\")).find_next().text.strip()\n",
    "        \n",
    "        #Transform to int kilometros\n",
    "        car_dict[\"kilometros\"] = int(\"\".join(re.findall(\"\\d\", car_dict[\"kilometros\"])))\n",
    "        \n",
    "        #car body type\n",
    "        car_dict[\"carroceria\"] =  car_info_bs.find(\n",
    "            \"ul\", class_=\"datos-basicos-ficha\").find(\n",
    "            text = re.compile(\"Carrocería\")).find_next().text.strip()\n",
    "        \n",
    "        #transmission type\n",
    "        car_dict[\"cambio\"] =  car_info_bs.find(\n",
    "            \"ul\", class_=\"datos-basicos-ficha\").find(\n",
    "            text = re.compile(\"Cambio\")).find_next().text.strip()\n",
    "        \n",
    "        #car power\n",
    "        car_dict[\"potencia\"] =  car_info_bs.find(\n",
    "            \"ul\", class_=\"datos-basicos-ficha\").find(\n",
    "            text = re.compile(\"Potencia\")).find_next().text\n",
    "        \n",
    "        #guarantee months\n",
    "        car_dict[\"garantia\"] =  car_info_bs.find(\n",
    "            \"ul\", class_=\"datos-basicos-ficha\").find(\n",
    "            text = re.compile(\"Garantía\")).find_next().text[0:2]\n",
    "        \n",
    "        #color\n",
    "        car_dict[\"color\"] =  car_info_bs.find(\n",
    "            \"ul\", class_=\"datos-basicos-ficha\").find(\n",
    "            text = re.compile(\"Color\")).find_next().text.strip().replace(\"\\n\",\"\").replace(\"  \",\"\")\n",
    "        \n",
    "        #Car environmental labeling\n",
    "        try:\n",
    "            car_dict[\"distintivo\"] =  car_info_bs.find(\n",
    "                \"span\",class_=\"icon icon-info\").find_next().text\n",
    "        except:\n",
    "            car_dict[\"distintivo\"] = 'NULL'\n",
    "        \n",
    "        return car_dict\n",
    "    \n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362b6f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define starting url for scrapping\n",
    "start = \"https://www.autocasion.com/coches-ocasion?page=\"\n",
    "\n",
    "#define user_agents pool\n",
    "user_agents = ['Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36',\n",
    "               'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36',\n",
    "               'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36',\n",
    "               'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/602.2.14 (KHTML, like Gecko) Version/10.0.1 Safari/602.2.14',\n",
    "               'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36',\n",
    "               'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.98 Safari/537.36',\n",
    "               'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.98 Safari/537.36',\n",
    "               'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36',\n",
    "               'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36',\n",
    "               'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:50.0) Gecko/20100101 Firefox/50.0'\n",
    "              ]\n",
    "\n",
    "#Path to save the csv\n",
    "path = 'aocars.csv'\n",
    "\n",
    "#number of pages to scrap\n",
    "PAGES = 625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1851005",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the CSV file and create a csv\n",
    "with open(path, 'w', newline='') as outfile:\n",
    "    \n",
    "    writer = csv.writer(outfile, delimiter=';')\n",
    "    \n",
    "    #Write header for each characteristic\n",
    "\n",
    "    writer.writerow([\"link\",\n",
    "                     \"marca\",\n",
    "                     \"provincia\",\n",
    "                     \"modelo\",\n",
    "                     \"precio\",\n",
    "                     \"matriculacion\",\n",
    "                     \"combustible\",\n",
    "                     \"kilometros\",\n",
    "                     \"carroceria\",\n",
    "                     \"cambio\",\n",
    "                     \"potencia\",\n",
    "                     \"garantia\",\n",
    "                     \"color\",\n",
    "                     \"distintivo\"\n",
    "                    ])\n",
    "    \n",
    "    print(\"CSV File created, starting scrapping\")\n",
    "    \n",
    "    #Iterate through website pages\n",
    "    for page in range(1, PAGES + 1):\n",
    "        \n",
    "        if page%10 == 0:\n",
    "            print(\"Currently scrapping page\", page)\n",
    "            \n",
    "        start = start + str(page)\n",
    "        #Scrap all car announcements urls from the current page\n",
    "        for car_url in scrap_car_urls(start, user_agents):\n",
    "            try:\n",
    "                detalles = scrap_car_details(car_url, user_agents)\n",
    "                writer.writerow(detalles.values())\n",
    "                \n",
    "            except:\n",
    "                print(\"Error in page\",page, \" and link\", car_url)\n",
    "                continue\n",
    "                \n",
    "print(\"Web Scrapping has ended. Check the generated CSV file\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
