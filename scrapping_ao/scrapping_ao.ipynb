{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25c74e22",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1685956833801,
     "user": {
      "displayName": "Javier Quesada García",
      "userId": "08428804086464618311"
     },
     "user_tz": -120
    },
    "id": "25c74e22"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random \n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "l0Z_xOP7gRD8",
   "metadata": {
    "id": "l0Z_xOP7gRD8"
   },
   "outputs": [],
   "source": [
    "\n",
    "def scrap_request(url, user_agents):\n",
    "    \"\"\"\n",
    "    Creates a requests.Session and then\n",
    "    makes a GET request using the provided 'url' and 'user_agents'\n",
    "    It will pick a random user_agent from the user_agents list provided\n",
    "    It has a built-in random delay to avoid bot detection when web scrapping\n",
    "    It returns the session.get request\n",
    "    Will return None if some exception arises\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        random_user_agent = random.choice(user_agents)\n",
    "\n",
    "        session = requests.Session()\n",
    "\n",
    "        response = session.get(url, headers = {\"User-Agent\":random_user_agent} )\n",
    "\n",
    "        delay = random.uniform(1,3)\n",
    "\n",
    "        time.sleep(delay)\n",
    "\n",
    "        return response\n",
    "    \n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e615c7c",
   "metadata": {
    "id": "1e615c7c"
   },
   "outputs": [],
   "source": [
    "def scrap_car_urls(url, user_agents):\n",
    "    \"\"\"\n",
    "    This functions scraps each car announcement URL from a autocasion.com page\n",
    "    It uses scrap_request function for the GET request\n",
    "    It will return a list with the complete link for each car announcement\n",
    "    Will return None if some exception arises\n",
    "    \"\"\"\n",
    "    try:\n",
    "        page_bs = BeautifulSoup(scrap_request(url, user_agents).text, \"lxml\")\n",
    "        \n",
    "        cars_list = page_bs.find_all(\"article\", {'class': re.compile(\"anuncio\")})\n",
    "        \n",
    "        starting_url = \"https://www.autocasion.com\"\n",
    "        \n",
    "        links_list = [starting_url + link.find(\"a\")[\"href\"]\n",
    "                  for link in cars_list]\n",
    "        \n",
    "        return links_list\n",
    "    \n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fe81ddb",
   "metadata": {
    "id": "9fe81ddb",
    "outputId": "3b514d50-64d3-4636-f4b4-162e4c45beb2"
   },
   "outputs": [],
   "source": [
    "def scrap_car_details(url, user_agents):\n",
    "    \"\"\"\n",
    "    This function scraps a car announcement url from autocasion.com\n",
    "    It uses scrap_request function for the GET request\n",
    "    It returns a dictionary with its main characteristics (brand, model, transmission...)\n",
    "    Will return None if some exception arises\n",
    "    \"\"\"\n",
    "    try:\n",
    "        car_dict = {}\n",
    "        \n",
    "        car_info_bs = BeautifulSoup(scrap_request(url, user_agents).text, \"lxml\")\n",
    "        \n",
    "        #car announcement url\n",
    "        car_dict[\"link\"] = url\n",
    "        \n",
    "        #car brand\n",
    "        car_dict[\"marca\"] = car_info_bs.find(\"div\", class_=\"bloque paginacion-ficha\").find_all(itemprop=\"name\")[1].text\n",
    "        \n",
    "        #province where the car is being sold\n",
    "        car_dict[\"provincia\"] = car_info_bs.find(\"div\", class_=\"bloque paginacion-ficha\").find_all(itemprop=\"name\")[2].text\n",
    "        \n",
    "        #car model\n",
    "        car_dict[\"modelo\"] = car_info_bs.find(\"div\", class_=\"bloque paginacion-ficha\").find_all(itemprop=\"name\")[3].text\n",
    "        \n",
    "        #car price\n",
    "        car_dict[\"precio\"] = int(\"\".join(re.findall(\"\\d\",car_info_bs.find(\"div\", class_=\"precio\").find(\"span\").text.strip())))\n",
    "        \n",
    "        #year in which the car was registrated\n",
    "        car_dict[\"matriculacion\"] = car_info_bs.find(\n",
    "            \"ul\", class_=\"datos-basicos-ficha\").find(\n",
    "            text= re.compile(\"Fecha de matriculación\")\n",
    "        ).find_next().text[-4:]\n",
    "        \n",
    "        #fuel type\n",
    "        car_dict[\"combustible\"] =  car_info_bs.find(\n",
    "            \"ul\", class_=\"datos-basicos-ficha\").find(\n",
    "            text = re.compile(\"Combustible\")).find_next().text.strip()\n",
    "        \n",
    "        #car km\n",
    "        car_dict[\"kilometros\"] =  car_info_bs.find(\n",
    "            \"ul\", class_=\"datos-basicos-ficha\").find(\n",
    "            text = re.compile(\"Kilómetros\")).find_next().text.strip()\n",
    "        \n",
    "        #Transform to int kilometros\n",
    "        car_dict[\"kilometros\"] = int(\"\".join(re.findall(\"\\d\", car_dict[\"kilometros\"])))\n",
    "        \n",
    "        #car body type\n",
    "        car_dict[\"carroceria\"] =  car_info_bs.find(\n",
    "            \"ul\", class_=\"datos-basicos-ficha\").find(\n",
    "            text = re.compile(\"Carrocería\")).find_next().text.strip()\n",
    "        \n",
    "        #transmission type\n",
    "        car_dict[\"cambio\"] =  car_info_bs.find(\n",
    "            \"ul\", class_=\"datos-basicos-ficha\").find(\n",
    "            text = re.compile(\"Cambio\")).find_next().text.strip()\n",
    "        \n",
    "        #car power\n",
    "        car_dict[\"potencia\"] =  car_info_bs.find(\n",
    "            \"ul\", class_=\"datos-basicos-ficha\").find(\n",
    "            text = re.compile(\"Potencia\")).find_next().text\n",
    "        \n",
    "        #guarantee months\n",
    "        car_dict[\"garantia\"] =  car_info_bs.find(\n",
    "            \"ul\", class_=\"datos-basicos-ficha\").find(\n",
    "            text = re.compile(\"Garantía\")).find_next().text[0:2]\n",
    "        \n",
    "        #color\n",
    "        car_dict[\"color\"] =  car_info_bs.find(\n",
    "            \"ul\", class_=\"datos-basicos-ficha\").find(\n",
    "            text = re.compile(\"Color\")).find_next().text.strip().replace(\"\\n\",\"\").replace(\"  \",\"\")\n",
    "        \n",
    "        #Car environmental labeling\n",
    "        try:\n",
    "            car_dict[\"distintivo\"] =  car_info_bs.find(\n",
    "                \"span\",class_=\"icon icon-info\").find_next().text\n",
    "        except:\n",
    "            car_dict[\"distintivo\"] = 'NULL'\n",
    "        \n",
    "        return car_dict\n",
    "    \n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "362b6f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define starting url for scrapping\n",
    "start = \"https://www.autocasion.com/coches-ocasion?page=\"\n",
    "\n",
    "#define user_agents pool\n",
    "user_agents = ['Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36',\n",
    "               'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36',\n",
    "               'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36',\n",
    "               'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/602.2.14 (KHTML, like Gecko) Version/10.0.1 Safari/602.2.14',\n",
    "               'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36',\n",
    "               'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.98 Safari/537.36',\n",
    "               'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.98 Safari/537.36',\n",
    "               'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36',\n",
    "               'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36',\n",
    "               'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:50.0) Gecko/20100101 Firefox/50.0'\n",
    "              ]\n",
    "\n",
    "#Path to save the csv\n",
    "path = 'aocars.csv'\n",
    "\n",
    "#number of pages to scrap\n",
    "PAGES = 625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1851005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV File created, starting scrapping\n",
      "Currently scrapping page 10\n",
      "Currently scrapping page 20\n",
      "Currently scrapping page 30\n",
      "Currently scrapping page 40\n",
      "Currently scrapping page 50\n",
      "Currently scrapping page 60\n",
      "Currently scrapping page 70\n",
      "Currently scrapping page 80\n",
      "Currently scrapping page 90\n",
      "Currently scrapping page 100\n",
      "Currently scrapping page 110\n",
      "Currently scrapping page 120\n",
      "Currently scrapping page 130\n",
      "Currently scrapping page 140\n",
      "Currently scrapping page 150\n",
      "Currently scrapping page 160\n",
      "Currently scrapping page 170\n",
      "Currently scrapping page 180\n",
      "Currently scrapping page 190\n",
      "Currently scrapping page 200\n",
      "Currently scrapping page 210\n",
      "Currently scrapping page 220\n",
      "Currently scrapping page 230\n",
      "Currently scrapping page 240\n",
      "Currently scrapping page 250\n",
      "Currently scrapping page 260\n",
      "Currently scrapping page 270\n",
      "Currently scrapping page 280\n",
      "Currently scrapping page 290\n",
      "Error in page 291  and link https://www.autocasion.com/coches-segunda-mano/dacia-duster-ocasion/duster-1-2-tce-laureate-4x2-125-ref8936294\n",
      "Currently scrapping page 300\n",
      "Currently scrapping page 310\n",
      "Currently scrapping page 320\n",
      "Currently scrapping page 330\n",
      "Currently scrapping page 340\n",
      "Currently scrapping page 350\n",
      "Currently scrapping page 360\n",
      "Currently scrapping page 370\n",
      "Currently scrapping page 380\n",
      "Currently scrapping page 390\n",
      "Currently scrapping page 400\n",
      "Currently scrapping page 410\n",
      "Currently scrapping page 420\n",
      "Currently scrapping page 430\n",
      "Currently scrapping page 440\n",
      "Currently scrapping page 450\n",
      "Currently scrapping page 460\n",
      "Currently scrapping page 470\n",
      "Currently scrapping page 480\n",
      "Currently scrapping page 490\n",
      "Currently scrapping page 500\n",
      "Currently scrapping page 510\n",
      "Currently scrapping page 520\n",
      "Currently scrapping page 530\n",
      "Currently scrapping page 540\n",
      "Currently scrapping page 550\n",
      "Currently scrapping page 560\n",
      "Currently scrapping page 570\n",
      "Currently scrapping page 580\n",
      "Currently scrapping page 590\n",
      "Currently scrapping page 600\n",
      "Currently scrapping page 610\n",
      "Currently scrapping page 620\n",
      "Web Scrapping has ended. Check the generated CSV file\n"
     ]
    }
   ],
   "source": [
    "#Open the CSV file and create a csv\n",
    "with open(path, 'w', newline='') as outfile:\n",
    "    \n",
    "    writer = csv.writer(outfile, delimiter=';')\n",
    "    \n",
    "    #Write header for each characteristic\n",
    "\n",
    "    writer.writerow([\"link\",\n",
    "                     \"marca\",\n",
    "                     \"provincia\",\n",
    "                     \"modelo\",\n",
    "                     \"precio\",\n",
    "                     \"matriculacion\",\n",
    "                     \"combustible\",\n",
    "                     \"kilometros\",\n",
    "                     \"carroceria\",\n",
    "                     \"cambio\",\n",
    "                     \"potencia\",\n",
    "                     \"garantia\",\n",
    "                     \"color\",\n",
    "                     \"distintivo\"\n",
    "                    ])\n",
    "    \n",
    "    print(\"CSV File created, starting scrapping\")\n",
    "    \n",
    "    #Iterate through website pages\n",
    "    for page in range(1, PAGES + 1):\n",
    "        \n",
    "        if page%10 == 0:\n",
    "            print(\"Currently scrapping page\", page)\n",
    "            \n",
    "        start = start + str(page)\n",
    "        #Scrap all car announcements urls from the current page\n",
    "        for car_url in scrap_car_urls(start, user_agents):\n",
    "            try:\n",
    "                detalles = scrap_car_details(car_url, user_agents)\n",
    "                writer.writerow(detalles.values())\n",
    "                \n",
    "            except:\n",
    "                print(\"Error in page\",page, \" and link\", car_url)\n",
    "                continue\n",
    "                \n",
    "print(\"Web Scrapping has ended. Check the generated CSV file\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
